{
  "hash": "a9873fe8eb1a87e0b92b00557e928568",
  "result": {
    "markdown": "# Statistical Tests and Models\n\n\n## Tests for Exploratory Data Analysis\n\nA collection of functions are available from `scipy.stats`.\n\n+ Comparing the locations of two samples\n    - `ttest_ind`: t-test for two independent samples\n    - `ttest_rel`: t-test for paired samples\n\t- `ranksums`: Wilcoxon rank-sum test for two independent samples\n\t- `wilcoxon`: Wilcoxon signed-rank test for paired samples\n+ Comparing the locations of multiple samples\n    - `f_oneway`: one-way ANOVA\n\t- `kruskal`: Kruskal-Wallis H-test\n+ Tests for associations in contigency tables\n    - `chi2_contingency`: Chi-square test of independence of variables\n\t- `fisher_exact`:  Fisher exact test on a 2x2 contingency table\n+ Goodness of fit\n    - `goodness_of_fit`: distribution could contain unspecified parameters\n\t- `anderson`: Anderson-Darling test \n    - `kstest`: Kolmogorov-Smirnov test \n\t- `chisquare`: one-way chi-square test\n\t- `normaltest`: test for normality\n\n\nSince R has a richer collections of statistical functions, we can call \nR function from Python with `rpy2`. See, for example, a [blog on this\nsubject](https://rviews.rstudio.com/2022/05/25/calling-r-from-python-with-rpy2/).\n\n## Linear Model\n\nLet's simulate some data for illustrations.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport statsmodels.api as sm\n\nnobs = 200\nncov = 5\nnp.random.seed(123)\nx = np.random.random((nobs, ncov)) # Uniform over [0, 1)\nbeta = np.repeat(1, ncov)\ny = 2 + np.dot(x, beta) + np.random.normal(size = nobs)\n```\n:::\n\n\nCheck the shape of `y`:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ny.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\n(200,)\n```\n:::\n:::\n\n\nCheck the shape of `x`:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nx.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\n(200, 5)\n```\n:::\n:::\n\n\nThat is, the true linear regression model is\n$$\ny = 2 + x_1 + x_2 + x_3 + x_4 + x_5 + \\epsilon.\n$$\n\nA regression model for the observed data can be fitted as\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nxmat = sm.add_constant(x)\nmymod = sm.OLS(y, xmat)\nmyfit = mymod.fit()\nmyfit.summary()\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.309</td>\n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.292</td>\n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   17.38</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Mon, 13 Feb 2023</td> <th>  Prob (F-statistic):</th> <td>3.31e-14</td>\n</tr>\n<tr>\n  <th>Time:</th>                 <td>15:41:34</td>     <th>  Log-Likelihood:    </th> <td> -272.91</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>   200</td>      <th>  AIC:               </th> <td>   557.8</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>   194</td>      <th>  BIC:               </th> <td>   577.6</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>   \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th> <td>    1.8754</td> <td>    0.282</td> <td>    6.656</td> <td> 0.000</td> <td>    1.320</td> <td>    2.431</td>\n</tr>\n<tr>\n  <th>x1</th>    <td>    1.1703</td> <td>    0.248</td> <td>    4.723</td> <td> 0.000</td> <td>    0.682</td> <td>    1.659</td>\n</tr>\n<tr>\n  <th>x2</th>    <td>    0.8988</td> <td>    0.235</td> <td>    3.825</td> <td> 0.000</td> <td>    0.435</td> <td>    1.362</td>\n</tr>\n<tr>\n  <th>x3</th>    <td>    0.9784</td> <td>    0.238</td> <td>    4.114</td> <td> 0.000</td> <td>    0.509</td> <td>    1.448</td>\n</tr>\n<tr>\n  <th>x4</th>    <td>    1.3418</td> <td>    0.250</td> <td>    5.367</td> <td> 0.000</td> <td>    0.849</td> <td>    1.835</td>\n</tr>\n<tr>\n  <th>x5</th>    <td>    0.6027</td> <td>    0.239</td> <td>    2.519</td> <td> 0.013</td> <td>    0.131</td> <td>    1.075</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td> 0.810</td> <th>  Durbin-Watson:     </th> <td>   1.978</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.667</td> <th>  Jarque-Bera (JB):  </th> <td>   0.903</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td>-0.144</td> <th>  Prob(JB):          </th> <td>   0.637</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 2.839</td> <th>  Cond. No.          </th> <td>    8.31</td>\n</tr>\n</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n:::\n\n\nQuestions to review:\n\n+ How are the regression coefficients interpreted? Intercept?\n+ Why does it make sense to center the covariates?\n\n\nNow we form a data frame with the variables\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nimport pandas as pd\ndf = np.concatenate((y.reshape((nobs, 1)), x), axis = 1)\ndf = pd.DataFrame(data = df,\n                  columns = [\"y\"] + [\"x\" + str(i) for i in range(1,\n                  ncov + 1)])\ndf.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 200 entries, 0 to 199\nData columns (total 6 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   y       200 non-null    float64\n 1   x1      200 non-null    float64\n 2   x2      200 non-null    float64\n 3   x3      200 non-null    float64\n 4   x4      200 non-null    float64\n 5   x5      200 non-null    float64\ndtypes: float64(6)\nmemory usage: 9.5 KB\n```\n:::\n:::\n\n\nLet's use a formula to specify the regression model as in R, and fit\na robust linear model (`rlm`) instead of OLS. Note that the model specification\nand the function interface is similar to R.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nimport statsmodels.formula.api as smf\nmymod = smf.rlm(formula = \"y ~ x1 + x2 + x3 + x4 + x5\", data = df)\nmyfit = mymod.fit()\nmyfit.summary()\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```{=html}\n<table class=\"simpletable\">\n<caption>Robust linear Model Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>          <td>y</td>        <th>  No. Observations:  </th> <td>   200</td>\n</tr>\n<tr>\n  <th>Model:</th>                 <td>RLM</td>       <th>  Df Residuals:      </th> <td>   194</td>\n</tr>\n<tr>\n  <th>Method:</th>               <td>IRLS</td>       <th>  Df Model:          </th> <td>     5</td>\n</tr>\n<tr>\n  <th>Norm:</th>                <td>HuberT</td>      <th>                     </th>    <td> </td>  \n</tr>\n<tr>\n  <th>Scale Est.:</th>            <td>mad</td>       <th>                     </th>    <td> </td>  \n</tr>\n<tr>\n  <th>Cov Type:</th>              <td>H1</td>        <th>                     </th>    <td> </td>  \n</tr>\n<tr>\n  <th>Date:</th>           <td>Mon, 13 Feb 2023</td> <th>                     </th>    <td> </td>  \n</tr>\n<tr>\n  <th>Time:</th>               <td>15:41:34</td>     <th>                     </th>    <td> </td>  \n</tr>\n<tr>\n  <th>No. Iterations:</th>        <td>16</td>        <th>                     </th>    <td> </td>  \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>Intercept</th> <td>    1.8353</td> <td>    0.294</td> <td>    6.246</td> <td> 0.000</td> <td>    1.259</td> <td>    2.411</td>\n</tr>\n<tr>\n  <th>x1</th>        <td>    1.1254</td> <td>    0.258</td> <td>    4.355</td> <td> 0.000</td> <td>    0.619</td> <td>    1.632</td>\n</tr>\n<tr>\n  <th>x2</th>        <td>    0.9664</td> <td>    0.245</td> <td>    3.944</td> <td> 0.000</td> <td>    0.486</td> <td>    1.447</td>\n</tr>\n<tr>\n  <th>x3</th>        <td>    0.9995</td> <td>    0.248</td> <td>    4.029</td> <td> 0.000</td> <td>    0.513</td> <td>    1.486</td>\n</tr>\n<tr>\n  <th>x4</th>        <td>    1.3275</td> <td>    0.261</td> <td>    5.091</td> <td> 0.000</td> <td>    0.816</td> <td>    1.839</td>\n</tr>\n<tr>\n  <th>x5</th>        <td>    0.6768</td> <td>    0.250</td> <td>    2.712</td> <td> 0.007</td> <td>    0.188</td> <td>    1.166</td>\n</tr>\n</table><br/><br/>If the model instance has been used for another fit with different fit parameters, then the fit options might not be the correct ones anymore .\n```\n:::\n:::\n\n\nFor model diagnostics, one can check residual plots.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\nmyOlsFit = smf.ols(formula = \"y ~ x1 + x2 + x3 + x4 + x5\", data = df).fit()\nfig = plt.figure(figsize = (6, 6))\n## residual versus x1; can do the same for other covariates\nfig = sm.graphics.plot_regress_exog(myOlsFit, 'x1', fig=fig)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\neval_env: 1\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](stats_files/figure-html/cell-8-output-2.png){width=566 height=570}\n:::\n:::\n\n\nSee more on [residual diagnostics and specification\ntests](https://www.statsmodels.org/stable/stats.html#residual-diagnostics-and-specification-tests).\n\n\n## Generalized Linear Regression\n\nA linear regression model cannot be applied to presence/absence or\ncount data. \n\n\nBinary or count data need to be modeled under a generlized\nframework. Consider a binary or count variable $Y$ with possible\ncovariates $X$.  A generalized model describes a transformation $g$\nof the conditional mean $E[Y | X]$ by a linear predictor\n$X^{\\top}\\beta$. That is\n$$\ng( E[Y | X] ) = X^{\\top} \\beta.\n$$\nThe transformation $g$ is known as the link function.\n\n\nFor logistic regression with binary outcomes, the link function is\nthe logit function\n$$\ng(u) = \\log \\frac{u}{1 - u}, \\quad u \\in (0, 1).\n$$\n\nWhat is the interpretation of the regression coefficients in a\nlogistic regression? Intercept?\n\nA logistic regression can be fit with `statsmodels.api.glm`.\n\nLet's generate some binary data first by dichotomizing existing variables.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\ndf['yb' ] = np.where(df['y' ] > 2.5, 1, 0)\ndf['x1b'] = np.where(df['x1'] > 0.5, 1, 0)\n```\n:::\n\n\nFit a logistic regression for `y1b`.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nmylogistic = smf.glm(formula = 'yb ~ x1b + x2 + x3 + x4 + x5', data = df,\n                     family = sm.families.Binomial())\nmylfit = mylogistic.fit()\nmylfit.summary()\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```{=html}\n<table class=\"simpletable\">\n<caption>Generalized Linear Model Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>          <td>yb</td>        <th>  No. Observations:  </th>  <td>   200</td> \n</tr>\n<tr>\n  <th>Model:</th>                  <td>GLM</td>       <th>  Df Residuals:      </th>  <td>   194</td> \n</tr>\n<tr>\n  <th>Model Family:</th>        <td>Binomial</td>     <th>  Df Model:          </th>  <td>     5</td> \n</tr>\n<tr>\n  <th>Link Function:</th>         <td>Logit</td>      <th>  Scale:             </th> <td>  1.0000</td>\n</tr>\n<tr>\n  <th>Method:</th>                <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td> -30.654</td>\n</tr>\n<tr>\n  <th>Date:</th>            <td>Mon, 13 Feb 2023</td> <th>  Deviance:          </th> <td>  61.307</td>\n</tr>\n<tr>\n  <th>Time:</th>                <td>15:41:35</td>     <th>  Pearson chi2:      </th>  <td>  217.</td> \n</tr>\n<tr>\n  <th>No. Iterations:</th>          <td>7</td>        <th>  Pseudo R-squ. (CS):</th>  <td>0.05871</td>\n</tr>\n<tr>\n  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>Intercept</th> <td>   -0.4354</td> <td>    1.309</td> <td>   -0.333</td> <td> 0.739</td> <td>   -3.002</td> <td>    2.131</td>\n</tr>\n<tr>\n  <th>x1b</th>       <td>    1.2721</td> <td>    0.845</td> <td>    1.505</td> <td> 0.132</td> <td>   -0.384</td> <td>    2.928</td>\n</tr>\n<tr>\n  <th>x2</th>        <td>    2.6897</td> <td>    1.418</td> <td>    1.897</td> <td> 0.058</td> <td>   -0.089</td> <td>    5.469</td>\n</tr>\n<tr>\n  <th>x3</th>        <td>   -0.0537</td> <td>    1.270</td> <td>   -0.042</td> <td> 0.966</td> <td>   -2.542</td> <td>    2.435</td>\n</tr>\n<tr>\n  <th>x4</th>        <td>    2.6576</td> <td>    1.438</td> <td>    1.848</td> <td> 0.065</td> <td>   -0.160</td> <td>    5.476</td>\n</tr>\n<tr>\n  <th>x5</th>        <td>    1.8752</td> <td>    1.330</td> <td>    1.409</td> <td> 0.159</td> <td>   -0.732</td> <td>    4.483</td>\n</tr>\n</table>\n```\n:::\n:::\n\n\nIf we treat `y1b` as count data, a Poisson regression can be fitted.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nmyPois = smf.glm(formula = 'yb ~ x1b + x2 + x3 + x4 + x5', data = df,\n                 family = sm.families.Poisson())\nmypfit = myPois.fit()\nmypfit.summary()\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```{=html}\n<table class=\"simpletable\">\n<caption>Generalized Linear Model Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>          <td>yb</td>        <th>  No. Observations:  </th>  <td>   200</td> \n</tr>\n<tr>\n  <th>Model:</th>                  <td>GLM</td>       <th>  Df Residuals:      </th>  <td>   194</td> \n</tr>\n<tr>\n  <th>Model Family:</th>         <td>Poisson</td>     <th>  Df Model:          </th>  <td>     5</td> \n</tr>\n<tr>\n  <th>Link Function:</th>          <td>Log</td>       <th>  Scale:             </th> <td>  1.0000</td>\n</tr>\n<tr>\n  <th>Method:</th>                <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td> -199.55</td>\n</tr>\n<tr>\n  <th>Date:</th>            <td>Mon, 13 Feb 2023</td> <th>  Deviance:          </th> <td>  17.091</td>\n</tr>\n<tr>\n  <th>Time:</th>                <td>15:41:35</td>     <th>  Pearson chi2:      </th>  <td>  8.99</td> \n</tr>\n<tr>\n  <th>No. Iterations:</th>          <td>4</td>        <th>  Pseudo R-squ. (CS):</th> <td>0.002487</td>\n</tr>\n<tr>\n  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>Intercept</th> <td>   -0.2045</td> <td>    0.288</td> <td>   -0.710</td> <td> 0.477</td> <td>   -0.769</td> <td>    0.360</td>\n</tr>\n<tr>\n  <th>x1b</th>       <td>    0.0445</td> <td>    0.146</td> <td>    0.304</td> <td> 0.761</td> <td>   -0.242</td> <td>    0.331</td>\n</tr>\n<tr>\n  <th>x2</th>        <td>    0.0965</td> <td>    0.249</td> <td>    0.387</td> <td> 0.699</td> <td>   -0.392</td> <td>    0.585</td>\n</tr>\n<tr>\n  <th>x3</th>        <td>   -0.0088</td> <td>    0.254</td> <td>   -0.035</td> <td> 0.972</td> <td>   -0.507</td> <td>    0.490</td>\n</tr>\n<tr>\n  <th>x4</th>        <td>    0.1072</td> <td>    0.267</td> <td>    0.402</td> <td> 0.688</td> <td>   -0.416</td> <td>    0.630</td>\n</tr>\n<tr>\n  <th>x5</th>        <td>    0.0750</td> <td>    0.253</td> <td>    0.296</td> <td> 0.767</td> <td>   -0.421</td> <td>    0.571</td>\n</tr>\n</table>\n```\n:::\n:::\n\n\n",
    "supporting": [
      "stats_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}