## Bagging vs. Boosting
**By Nathan Nhan**


### Introduction

Before we talk about Bagging and Boosting we first must talk about ensemble
learning. Ensemble learning is a technique used in machine-learning where we
have multiple models (often times called "weak learners") that are trained to
solve the same problem and then combined to obtain better results that they
could individually. With this, we can obtain more accurate and more robust
models for our data using this technique. 


Bagging and boosting are two types of ensemble learning techniques. They
decrease the variance of a single estimate as they combine multiple estimates
from different models to create a model with higher stability. Additionally,
ensemble learning techniques increase the stability of the final model by
reducing faactors of error in our models such as unnecessary noise, bias, and
variance that we might find hurts the accuracy of our model. Specifically:


* Bagging helps decrease the model’s variance and prevent over-fitting.
* Boosting helps decrease the model’s bias.


### Bagging


Bagging, which stands for bootrap aggregation, is a ensemble learning algorithm
designed to improve the stability and accuracy of algorithms used in statistical
classification and regression. In Bagging, multiple homogenous algorithms are
trained independently and combined afterward to determine the model’s
average. It works like this: 


* From the original dataset, multiple subsets are created, selecting
  observations with replacement.
* On each of these subsets, a base learner (weak learner) is created for each
* Next, all the independent models are run in parallel with one another
* The final predictions are determined by combining the predictions from all the
  models.
  


![Visualization on how the bagging algorithm works](images/Bagging.png)


The benefits of using bagging algorithms are that 
* Bagging algorithms reduce bias and variance errors.
* Bagging algorithms can handle overfitting (when a model works with a training
  dataset but fails with the actual testing dataset).
* Bagging can easily be implemented and produce more robust models.


#### Illustration

First we must load the dataset. For this topic I will be generating a madeup
dataset shown below. It is recommended to make sure the dataset has no missing
values as datasets with missing values leads to inconsistent results and poor
model performance.


```{python}
import pandas as pd
import numpy as np
import random

length = 1000
random.seed(0)

# Generate a random dataset
data = {
        'Age': [random.randint(10, 80) for x in range(length)],
        'Weight': [random.randint(110, 250) for x in range(length)],
        'Height': [random.randint(55, 77) for x in range(length)],
        'Average BPM': [random.randint(70, 100) for x in range(length)],
        'Amount of Surgeries': [random.randint(0, 3) for x in range(length)],
        'Blood Pressure': [random.randint(100, 180) for x in range(length)],
        }

data['Healthy?'] = np.nan
df = pd.DataFrame(data)


# Generate a random response variable displaying "1" for healthy and "0" for unhealthy
for index, row in df.iterrows():
    if row['Blood Pressure'] < 110 and row['Average BPM'] > 80:
        df.at[index, 'Healthy?'] = random.choice([0, 1, 1, 1, 0, 0, 0])
    else:
        df.at[index, 'Healthy?'] = random.choice([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1])


df
```


Next, we need to specify the x and y variables where the x-variable will hold
all the input columns containing numbers and y-variable will contain the output
column.

```{python}
X = df.drop(["Healthy?"], axis="columns")
y = df['Healthy?']

print(X)
```

Next we must scale our data. Dataset scaling is transforming the dataset to fit
within a specific range. This ensures that no data point is left out during
model training. In the example giving we will use the `StandardScaler` method to
scale our dataset.


```{python}
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

print(X)
print(X_scaled)
```

After scaling the dataset, we can split it. We will split the scaled dataset
into two subsets: training and testing. To split the dataset, we will use the
train_test_split method. We will be using the default splitting ratio for the
`train_test_split` method which means that 80% of the data will be the training
set and 20% the testing set.


```{python}
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, stratify=y, random_state=0)
```

Now that we have split our data, we can now perform our classification. The
`BaggingClassifier` classifier will perform all the bagging steps and build an
optimized model based on our data. The `BaggingClassifier` will fit the
weak/base learners on the randomly sampled subsets. The listed parameters are as
follows: 

* `estimator` - this parameter takes the algoithm we want to use, in this
  example we use the `DecisionTreeClassifier` for our weak learners.
* `n_estimators` - this parameter takes the amount of weak learners we want to
  use.
* `max_samples` - this parameter represents The maximum number of data that is
  sampled from the training set. We use 80% of the training dataset for
  resampling.
* `bootstrap` - this parameter allows for resampling of the training dataset
  without replacement when set to `True`.
* `oob_score` - this parameter is used to compute the model’s accuracy score
  after training.
* `random_state` - Seed used by the random number generator so we can reproduce
  out results


```{python}
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier

bag_model = BaggingClassifier(
estimator=DecisionTreeClassifier(), # DecisionTreeClassifier() if estimator is not defined
n_estimators=100, 
bootstrap=True,
oob_score=True,
random_state = 0
)

bag_model.fit(X_train, y_train)
AccScore = bag_model.oob_score_
print("Accuracy Score for Bagging Classifier: " + str(AccScore))
```

We can find if overfitting occurs when we get a lower accuracy when using the
testing dataset. We can find this by running the following code: 

```{python}
if bag_model.score(X_test, y_test) < AccScore:
    print('Overfitting has occurred since the testing dataset has a lower accuracy than the training dataset')
else:
    print("No overfitting has occurred")
```

Comparing this to the non-bagging algorithm like K-fold cross-validation: 

```{python}
from sklearn.model_selection import cross_val_score

scores = cross_val_score(DecisionTreeClassifier(), X, y, cv=5)
print("Accuracy Score for Bagging Classifier: " + str(scores.mean())) 
```

#### Example: Random Forest Classifier

The Random Forest Classifier algorithm is a typical example of a bagging
algorithm. Random Forests uses bagging underneath to sample the dataset with
replacement randomly. Random Forests samples not only data rows but also
columns. It also follows the bagging steps to produce an aggregated final model.

```{python}
from sklearn.ensemble import RandomForestClassifier

clf = RandomForestClassifier(
n_estimators=100, 
bootstrap = True,
oob_score=True,
random_state = 0
)

clf = clf.fit(X, y)

print("Accuracy Score for Random Forest Regression Algoritm Classifier: " + str(clf.oob_score_)) 
```

### Boosting

The Boosting algorithm is a type of ensemble learning algorithm that works by
training weak models sequentially. First, a model is built from the training
data. Then an additional model is built which tries to correct the errors
present in the first model. When an input is misclassified by a model, its
weight is increased so that next model is more likely to classify it
correctly. This procedure is continued and models are added until either the
complete training data set is predicted correctly or the maximum number of n
models are added. 


It works step-by-step like this: 

* A subset is created from the original dataset, where initially all the data
  points are given equal weights.
* A base model is created on this subset which is used to make predictions on
  the whole dataset.
* Errors are calculated by comparing the actual values vs. the predicted
  values. The observations that are incorrectly predicted, are given higher
  weights. 
* An additional model is created and predictions are made on the dataset, trying
  to correct the errors of the previous model
* The cycle will repeat until the maximum number of n models are added.
* The final model (strong model) is the weighted mean of all the models (weak
  model).


![Visualization on how the boosting algorithm works](images/Boosting.png)


#### Example: AdaBoost
The AdaBoost algorithm, which is short for "Adaptive Boosting," was one of the
first boosting methods that saw an increase in accuracy and speed performance of
models. AdaBoost focuses on enhnacement in performance in areas where the first
iteration of the model fails.


We can see an implmentation of this algorithm below:

The `AdaBoostClassifier` method has some of the following parameters:

* `estimator` - this parameter takes the algoithm we want to use, in this
  example we use the `DecisionTreeClassifier` for our weak learners.
* `n_estimators` - this parameter takes the amount of weak learners we want to
  use.
* `learning_rate` - this parameter learning rate reduces the contribution of the
  classifier by this value. It has a default value of 1.
* `random_state` - Seed used by the random number generator so we can reproduce
  out results.
  

```{python}
from sklearn.ensemble import AdaBoostClassifier

adaboost = AdaBoostClassifier(
    estimator = DecisionTreeClassifier(),
    n_estimators = 100, 
    learning_rate = 0.2,
    random_state = 0
    )

adaboost.fit(X_train, y_train)
score = adaboost.score(X_test, y_test)
print("Accuracy Score for Adaboost Classifier: " + str(score)) 
```

#### Example: XGBoost

`XGBoost` is widely considered as one of the most important boosting methods for
its advantages over other boosting algorithms.


Other boosting algorithms typically use gradiant descent to find the minima of
the `n` features mapped in `n` dimensional space. However, `XGBoost` instead
uses the a mathematical technique called the Newton--Raphson method which uses
the second derivative of the function which provides curvature information in
contract to algorithms using gradiant descent which only use the first
derivative.


`XGBoost` has its own python library called `xgboost` just to itself for which
we wrote the example code below.


The `XGBClassifer` contains some of the following parameters:

* `learning_rate` - this parameter takes the amount of weak learners we want to
  use.
* `random_state` - Random number seed.
* `importance_type` - The feature to focus on; either gain, weight, cover,
  total_gain or total_cover.


```{python}
from xgboost import XGBClassifier
xgboost = XGBClassifier(
    n_estimators = 1000, 
    learning_rate = 0.05,
    random_state = 0)

xgboost.fit(X_train, y_train)
score_xgb = xgboost.score(X_test,y_test)
print("Accuracy Score for XGBoost Classifier: " + str(score_xgb)) 

```


### Comparison

Now with all that being said, which should you use? Bagging or Boosting?


Both Bagging and Boosting combine several estimates from different models so
both will turn out a model with higher stability. 


If you find that the problem when using a single model gets a high error,
bagging will rarely be better but on the other hand boosting will generate a
combined model with lower errors.


Conversely, if your single model is over-fitting then typically bagging is the
best option and boosting will not help for over-fitting. Therefore, bagging iw
what you want.


#### Similarites 

Below is a set of similarites between Bagging and Boosting:

* Both are ensemble learning methods to get N models all from one individual model

* Both use random sampling to generate several random subsets

* Both make the final decision by averaging and combining the N learners (or taking the majority of them i.e Majority Voting).

* Both reduce variance and provide higher data stability than one individual model would.

#### Differences:

Below is a set of differences between Bagging and Boosting:

* Bagging combines predictions belonging to the same type while boosting is a
  way of combining predictions that belong to different types
* Bagging decreases variance while boosting decreases bias. If the classifier is
  unstable (high variance), then we using Bagging. If the classifier is stable
  and simple (high bias), then we should use Boosting.
* Bagging each model receives equal weight where in Boosting each model is
  weighted based on their performance.
* Bagging has models run in parallel, independent of one another while Boosting
  has them run sequentially so each model is dependent on the previous
* In Bagging different training data subsets are randomly generated with
  replacement segemnts of the original training dataset. In Boosting each new
  subsets contains the elements that were misclassified by previous models.
* Bagging attempts to solve over-fitting problem while Boosting does not.


#### Conclusion: 

* We have explained what an ensemble method is and how Bagging and Boosting
  algorithms function.
* We have demonstrated the differences and similaries between the two ensemble
  methods: Bagging and Boosting.
* We showed how to implement both Bagging and Boosting algorithms into Python

### References

[Kaggle: Bagging vs. Boosting](https://www.kaggle.com/code/prashant111/bagging-vs-boostingaggingClassifier)

[Scikit.learn](https://scikit-learn.org/stable/modules/ensemble.html)

[Bagging algorithms in Python](https://www.section.io/engineering-education/implementing-bagging-algorithms-in-python/#how-bagging-works)

[Boosting Algorithms in Python](https://www.section.io/engineering-education/boosting-algorithms-python/#bagging-vs-boosting)

[GeeksforGeeks](https://www.geeksforgeeks.org/bagging-vs-boosting-in-machine-learning/)



